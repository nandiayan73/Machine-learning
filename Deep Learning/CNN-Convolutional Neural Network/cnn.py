import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
# ImageDataGenerator:

# ImageDataGenerator is a class provided by Keras that is used for data augmentation. 
# Data augmentation is a technique commonly used in computer vision tasks, particularly in deep learning, to artificially 
# increase the diversity of your training dataset. This helps the neural network 
# generalize better and improve its performance.


# Data Preprocessing-
# 1. Processing the training data set:


# After importing the ImageDataGenerator class, you can create an instance of 
# it and then use its methods to generate augmented versions of your input images. 
# These augmented versions can include various transformations such as rotations,
#  shifts, flips, zooms, and more. These transformed images can then be used for training deep
# learning models, particularly convolutional neural networks (CNNs),
# to improve their robustness and accuracy.
train_datagen=ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

# ***rescale***=> It is a parameter used to apply a scaling factor to the pixel values of the images. 
# It feature scales the image which is then can be used for deep learning purpose.



#using its flow_from_directory or flow methods to load and augment images from a
#directory during the training process.

# class_mode (str):

# This parameter specifies the type of label assignment to the images. It can take one of the following values:
# 'categorical' (default): Generates one-hot encoded labels for multiple classes.
# 'binary': Generates binary labels (0 or 1) for two classes.
# 'sparse': Generates integer labels for multiple classes.
# 'None': No labels are generated, and the generator will only yield images.


# batch_size (int):

# This is the number of images in each batch generated by the data generator.

# Augmenting images: It means that we have to increase the no. of images, with new size,
# new rotation, and create a batch which contains 32 images
train_set=train_datagen.flow_from_directory('dataset/training_set',
                                            target_size=(64,64),#target size of the image
                                            batch_size=32,
                                            class_mode='binary') 


# Processing the test set:

test_datagen=ImageDataGenerator(rescale=1./255)
test_set=test_datagen.flow_from_directory('dataset/test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')

#Building the CNN:
#  A Sequential model is a linear stack of layers where you can add layers one by one 
# in a sequential fashion, making it a straightforward way to build neural networks, 
# especially for feedforward neural networks like Convolutional Neural Networks (CNNs).
cnn=tf.keras.models.Sequential()


# filters (int):
# This parameter specifies the number of filters
#  (also known as kernels) that the convolutional 
# layer will use. Each filter is responsible for
#  learning different features in the input data. 
# In this case, you have set it to 32, meaning that
#  there will be 32 filters in this layer.

# kernel_size (int or tuple of two integers):
# kernel_size defines the dimensions of the convolutional kernel or filter. 
# It determines the size of the local region that the filter will slide over 
# in the input data to perform convolutions. In this code, the kernel size 
# is set to (3, 3), which means the filter will be a 3x3 matrix.

# Adding the 1st convolutional Layer to make feature map:

# step 1: Convulation:

cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation="relu",input_shape=[64,64,3]))                                            
# As we are working with color images so the input shape is 3.
# Kernel_size is the size of the matrix of the feature detector(3).

# step 2: Max Pooling:

cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))
# pool_size is the size of the matrix with which we pool the feature map.

# Adding the second convolutional layer:

cnn.add(tf.keras.layers.Conv2D(filters=32,activation="relu",kernel_size=3))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))


# Flattening:
cnn.add(tf.keras.layers.Flatten())

# Full Connection(Hidden Neurons):
cnn.add(tf.keras.layers.Dense(units=128,activation="relu"))

# The activation function used is ReLU (Rectified Linear Unit). 
# ReLU is a common activation function used in hidden layers of neural networks.
#  It introduces non-linearity to the model by transforming all negative input 
# values to zero and leaving positive values unchanged.

# Output Layer
cnn.add(tf.keras.layers.Dense(units=1,activation="sigmoid"))
# As the result is binary, so the activation function shouold be sigmoid.

# Training the cnn on the training dataset:

# Compiling the CNN
# cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Training the cnn:
cnn.fit(x=train_set,validation_data=test_set,epochs=25)

import numpy as np
from keras.preprocessing import image

test_image=image.load_img("dataset/predictions/test2.jpg",target_size=(64,64))
test_image=image.img_to_array(test_image)
test_image=np.expand_dims(test_image,axis=0)
result=cnn.predict(test_image)
train_set.class_indices

print("result is: "+str(result[0][0]))

if result[0][0] == 1:
    prediction = 'dog'
else:
    prediction = 'cat'
print(prediction)
# 32 * 250=8000

# Batch Size:

# Your chosen batch size is 32, which means that during each training iteration (epoch), your model will process 32 images together in parallel.
# An epoch refers to one complete pass through the entire dataset. Since you have 8000 images and you're processing them in batches of 32, it would take 8000 / 32 = 250 iterations (or 250 batches) to complete one epoch.
# A Convolutional Neural Network (CNN) is capable of separating images of cats and dogs by learning to recognize patterns and features that are characteristic of each animal. Here's a high-level overview of how CNNs work to distinguish between cats and dogs in images:

# Feature Learning:

# CNNs are composed of multiple layers, including convolutional layers, pooling layers, and fully connected layers. These layers work together to automatically extract hierarchical features from the input images.
# In the early layers of the network, low-level features such as edges, textures, and simple shapes are detected. These features are learned through convolutional operations where small filters (kernels) slide over the input images, capturing local patterns.
# As you move deeper into the network, higher-level features and more abstract representations are learned. The network starts recognizing complex structures, textures, and shapes that are relevant to distinguishing between cats and dogs.
# Pattern Recognition:

# As the CNN processes images during training, it learns to associate certain patterns and combinations of features with the presence of either cats or dogs. These patterns may include the shape of ears, the presence of whiskers, fur textures, and other distinctive characteristics of each animal.
# The network's convolutional layers are capable of learning and detecting these discriminative patterns through weight adjustments during training.
# Class Separation:

# The fully connected layers at the end of the CNN are responsible for making decisions based on the learned features. These layers combine the extracted features and make predictions about whether the image contains a cat or a dog.
# During training, the network's parameters (weights and biases) are adjusted to minimize the prediction errors. This is typically done using a loss function like categorical cross-entropy for classification tasks.
# Training Data:

# To train a CNN for cat vs. dog classification, you need a labeled dataset containing images of cats and dogs. Each image is associated with a class label (e.g., "cat" or "dog").
# The CNN is trained using this dataset by adjusting its parameters through backpropagation and gradient descent to minimize the classification error on the training data.
# Testing and Generalization:

# After training, the CNN can generalize its learned patterns to new, unseen images. When presented with an image of a cat or a dog that it hasn't encountered before, it uses its learned features and patterns to make predictions.
# The model's performance on a separate test dataset (unseen during training) provides an evaluation of its ability to correctly classify cats and dogs in real-world scenarios.
# In summary, a CNN separates images of cats and dogs by learning to recognize distinctive features and patterns associated with each class during the training process. It leverages the hierarchical feature learning capabilities of its layers to make accurate predictions on new and unseen images.